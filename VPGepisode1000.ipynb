{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ninja18/policy_gradients/blob/master/VPGepisode1000.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deUSsvl4y1QI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "fe3ae145-73d1-4553-edc6-7169dfa22207"
      },
      "source": [
        "!pip install torch\n",
        "!pip install gym"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.17.4)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.15.4)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.2.2)\n",
            "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.17.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.12.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from gym) (4.1.2.30)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WInkAjSiy7Df",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class MLPAgent(nn.Module):\n",
        "    def __init__(self, output_size,\n",
        "                 input_size,\n",
        "                 layer_sizes=[128, 128],\n",
        "                 dropout_rate=0.5):\n",
        "        super(MLPAgent, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        layer_sizes.insert(0, self.input_size)\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            self.layers.append(nn.Linear(layer_sizes[i], layer_sizes[i + 1], bias=False))\n",
        "            self.layers.append(nn.ReLU(inplace=True))\n",
        "        self.layers.append(nn.Dropout(p=dropout_rate))\n",
        "        self.layers.append(nn.Linear(layer_sizes[-1], output_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, self.input_size)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return F.softmax(x)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7m0gPOpFzlm5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f2dffd0a-a420-4b65-813b-fb711c6c07ff"
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83qw6jMIzqIA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "import gym\n",
        "import torch\n",
        "from torch import optim\n",
        "from torch.distributions import Categorical\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from google.colab import files\n",
        "\n",
        "# from src.VanillaPolicyGradient.agent import MLPAgent\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "\n",
        "    def __init__(self, env, obs_dims,\n",
        "                 agent, lr, epoch,\n",
        "                 gamma=0.95, batch_size=1,\n",
        "                 custom_actions=None,\n",
        "                 model_file=\"model.pt\", save_per_epoch=1,\n",
        "                 render=False):\n",
        "        self.env = env\n",
        "        self.dev = torch.device(\"cuda:0\")\n",
        "        self.obs_dims = obs_dims\n",
        "        self.obs_transform = None  # Todo get transform\n",
        "        # if len(self.obs_dims) > 1:\n",
        "        #     self.obs_transform = transforms.Compose([\n",
        "        #         transforms.ToPILImage(),\n",
        "        #         transforms.Grayscale(num_output_channels=self.obs_dims[2]),\n",
        "        #         transforms.Resize(self.obs_dims[:2]),\n",
        "        #         transforms.ToTensor(),\n",
        "        #     ])\n",
        "        self.agent = agent\n",
        "        self.agent.to(self.dev)\n",
        "        self.optimizer = optim.Adam(self.agent.parameters(), lr=lr)\n",
        "        self.custom_actions = custom_actions\n",
        "        self.epoch = epoch\n",
        "        self.current_epoch = 1\n",
        "        self.gamma = gamma\n",
        "        self.batch_size = batch_size\n",
        "        self.model_file = model_file\n",
        "        self.render = render\n",
        "        self.save_per_epoch = save_per_epoch\n",
        "        self.writer = SummaryWriter()\n",
        "        self.load_model()\n",
        "        self.log_probs = []\n",
        "        self.rewards = []\n",
        "\n",
        "    def load_model(self):\n",
        "        if os.path.exists(self.model_file):\n",
        "            checkpoint = torch.load(self.model_file)\n",
        "            self.agent.load_state_dict(checkpoint['model_state_dict'])\n",
        "            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "            self.current_epoch = checkpoint['epoch']\n",
        "            if self.current_epoch == self.epoch:\n",
        "                self.epoch += self.epoch\n",
        "            print(f\"Loaded existing model continuing from epoch {self.current_epoch}\")\n",
        "\n",
        "    def act(self, obs):\n",
        "        if len(obs.shape) == 1:\n",
        "            probs = self.agent(torch.from_numpy(obs).type(torch.FloatTensor))\n",
        "        else:\n",
        "            probs = self.agent(self.pre_process(obs))\n",
        "\n",
        "        dis = Categorical(probs)\n",
        "        action = dis.sample()\n",
        "        self.log_probs.append(dis.log_prob(action))\n",
        "        if self.custom_actions is not None:\n",
        "            action = self.custom_actions[action.item()]\n",
        "        return self.env.step(action.item())\n",
        "\n",
        "    def pre_process(self, obs):  # Todo preprocess in torch\n",
        "        # return self.obs_transform(obs).unsqueeze(0)\n",
        "        obs = obs[35:195]  # crop\n",
        "        obs = obs[::2, ::2, 0]  # downsample by factor of 2\n",
        "        obs[obs == 144] = 0  # erase background (background type 1)\n",
        "        obs[obs == 109] = 0  # erase background (background type 2)\n",
        "        obs[obs != 0] = 1  # everything else (paddles, ball) just set to 1\n",
        "        return torch.tensor(obs).type(torch.FloatTensor).to(self.dev)\n",
        "\n",
        "    def train(self):\n",
        "        self.agent.train()\n",
        "        for i in range(self.current_epoch, (self.epoch * self.batch_size) + 1):\n",
        "            obs = self.env.reset()\n",
        "            while True:\n",
        "                obs, rew, done, _ = self.act(obs)\n",
        "                self.rewards.append(rew)\n",
        "                if self.render:\n",
        "                    self.env.render()\n",
        "                if done:\n",
        "                    if i % self.batch_size == 0:\n",
        "                        loss = self.update_policy()\n",
        "                        total_reward = sum(self.rewards) / self.batch_size\n",
        "                        epoch = i // self.batch_size\n",
        "                        print(f\"Epoch: {epoch}\"\n",
        "                              f\"  Loss: {loss.item()}\"\n",
        "                              f\"  Total Average Reward: {total_reward}\")\n",
        "                        self.writer.add_scalar(\"Loss\", loss, epoch)\n",
        "                        self.writer.add_scalar(\"reward_per_episode\", total_reward, epoch)\n",
        "                        self.writer.flush()\n",
        "                        self.rewards.clear()\n",
        "                        self.log_probs = []\n",
        "                    break\n",
        "\n",
        "            if i % self.save_per_epoch == 0:\n",
        "                torch.save({\n",
        "                    'epoch': i,\n",
        "                    'model_state_dict': self.agent.state_dict(),\n",
        "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                }, self.model_file)\n",
        "            if i % 1000 == 0:\n",
        "                files.download(\"model.pt\")\n",
        "\n",
        "        self.env.close()\n",
        "\n",
        "    def update_policy(self):\n",
        "        norm_rewards = self.discounted_rewards()\n",
        "        cumulative_reward = - torch.cat(self.log_probs).to(self.dev) * norm_rewards\n",
        "        loss = torch.sum(cumulative_reward, -1)\n",
        "        # viz = make_dot(loss)\n",
        "        # viz.view()\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss\n",
        "\n",
        "    def discounted_rewards(self):\n",
        "        reward_accumulator = 0\n",
        "        discounted_rewards = []\n",
        "        for r in reversed(self.rewards):\n",
        "            if r != 0:\n",
        "                reward_accumulator = 0\n",
        "            reward_accumulator = r + reward_accumulator * self.gamma\n",
        "            discounted_rewards.insert(0, reward_accumulator)\n",
        "        discounted_rewards = torch.tensor(discounted_rewards, dtype=torch.float32).to(self.dev)\n",
        "        norm_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-06)\n",
        "        return norm_rewards\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bx-xW05x0K06",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "84b2d892-944e-4208-8599-f361d445c780"
      },
      "source": [
        "import time\n",
        "env = gym.make(\"Pong-v0\")\n",
        "agent = MLPAgent(2, 6400, layer_sizes=[200])\n",
        "trainer = Trainer(env, [80, 80, 1], agent, 0.0005, 1,\n",
        "                      custom_actions=torch.tensor([2, 3], dtype=torch.int16),\n",
        "                      save_per_epoch=1,\n",
        "                      gamma=0.99,\n",
        "                      batch_size=10, render=False)\n",
        "start = time.time()\n",
        "trainer.train()\n",
        "print(f\"One iteration takes : {time.time() - start} seconds\")\n",
        "\n",
        "!ls -al"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded existing model continuing from epoch 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1  Loss: -1.1816844940185547  Total Average Reward: -2.1\n",
            "One iteration takes : 37.68473505973816 seconds\n",
            "total 15028\n",
            "drwxr-xr-x 1 root root     4096 Dec 25 07:22 .\n",
            "drwxr-xr-x 1 root root     4096 Dec 25 06:29 ..\n",
            "drwxr-xr-x 1 root root     4096 Dec 18 16:52 .config\n",
            "-rw-r--r-- 1 root root 15366560 Dec 25 07:23 model.pt\n",
            "drwxr-xr-x 4 root root     4096 Dec 25 07:23 runs\n",
            "drwxr-xr-x 1 root root     4096 Dec 18 16:52 sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlTVRPHr04JX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "06955565-6ffa-4ab4-fe47-4b07121d2351"
      },
      "source": [
        "  env = gym.make(\"Pong-v0\")\n",
        "  # torch.set_default_dtype(torch.float16)\n",
        "  agent = MLPAgent(2, 6400, layer_sizes=[200])\n",
        "  trainer = Trainer(env, [80, 80, 1], agent, 0.0005, 2000,\n",
        "                    custom_actions=torch.tensor([2, 3], dtype=torch.int8),\n",
        "                    save_per_epoch=100,\n",
        "                    gamma=0.99,\n",
        "                    batch_size=5, render=False)\n",
        "  trainer.train()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1  Loss: 0.8826713562011719  Total Average Reward: -20.2\n",
            "Epoch: 2  Loss: -2.1915283203125  Total Average Reward: -20.6\n",
            "Epoch: 3  Loss: -0.6884956359863281  Total Average Reward: -20.0\n",
            "Epoch: 4  Loss: -5.493862152099609  Total Average Reward: -19.8\n",
            "Epoch: 5  Loss: -1.8027667999267578  Total Average Reward: -20.6\n",
            "Epoch: 6  Loss: -5.0925140380859375  Total Average Reward: -20.4\n",
            "Epoch: 7  Loss: -4.151176452636719  Total Average Reward: -20.4\n",
            "Epoch: 8  Loss: 6.0616135597229  Total Average Reward: -20.4\n",
            "Epoch: 9  Loss: -6.976550102233887  Total Average Reward: -20.6\n",
            "Epoch: 10  Loss: -5.052742004394531  Total Average Reward: -20.2\n",
            "Epoch: 11  Loss: -5.033237457275391  Total Average Reward: -20.2\n",
            "Epoch: 12  Loss: -15.842080116271973  Total Average Reward: -19.8\n",
            "Epoch: 13  Loss: -7.010899543762207  Total Average Reward: -20.2\n",
            "Epoch: 14  Loss: -19.20419692993164  Total Average Reward: -20.0\n",
            "Epoch: 15  Loss: -13.464981079101562  Total Average Reward: -20.0\n",
            "Epoch: 16  Loss: -12.676006317138672  Total Average Reward: -20.0\n",
            "Epoch: 17  Loss: -17.05270004272461  Total Average Reward: -20.0\n",
            "Epoch: 18  Loss: -16.367416381835938  Total Average Reward: -20.0\n",
            "Epoch: 19  Loss: -15.170255661010742  Total Average Reward: -20.4\n",
            "Epoch: 20  Loss: -6.46619987487793  Total Average Reward: -20.4\n",
            "Epoch: 21  Loss: -14.473587036132812  Total Average Reward: -19.4\n",
            "Epoch: 22  Loss: -21.392337799072266  Total Average Reward: -20.4\n",
            "Epoch: 23  Loss: -13.138874053955078  Total Average Reward: -20.4\n",
            "Epoch: 24  Loss: -33.198997497558594  Total Average Reward: -20.0\n",
            "Epoch: 25  Loss: -6.176845550537109  Total Average Reward: -19.8\n",
            "Epoch: 26  Loss: -12.547672271728516  Total Average Reward: -19.8\n",
            "Epoch: 27  Loss: -21.377283096313477  Total Average Reward: -20.2\n",
            "Epoch: 28  Loss: -29.851734161376953  Total Average Reward: -20.4\n",
            "Epoch: 29  Loss: -8.766825675964355  Total Average Reward: -19.8\n",
            "Epoch: 30  Loss: -13.902833938598633  Total Average Reward: -20.6\n",
            "Epoch: 31  Loss: -6.308131217956543  Total Average Reward: -19.2\n",
            "Epoch: 32  Loss: -30.084300994873047  Total Average Reward: -19.8\n",
            "Epoch: 33  Loss: -10.161449432373047  Total Average Reward: -20.2\n",
            "Epoch: 34  Loss: -25.10137176513672  Total Average Reward: -19.4\n",
            "Epoch: 35  Loss: -19.26015281677246  Total Average Reward: -19.6\n",
            "Epoch: 36  Loss: -75.32320404052734  Total Average Reward: -19.8\n",
            "Epoch: 37  Loss: -27.54705810546875  Total Average Reward: -19.4\n",
            "Epoch: 38  Loss: 2.680161476135254  Total Average Reward: -20.0\n",
            "Epoch: 39  Loss: -21.250091552734375  Total Average Reward: -20.6\n",
            "Epoch: 40  Loss: -21.44558334350586  Total Average Reward: -19.6\n",
            "Epoch: 41  Loss: -26.14325714111328  Total Average Reward: -19.0\n",
            "Epoch: 42  Loss: -17.481639862060547  Total Average Reward: -19.6\n",
            "Epoch: 43  Loss: -29.17858123779297  Total Average Reward: -19.2\n",
            "Epoch: 44  Loss: -56.017189025878906  Total Average Reward: -19.2\n",
            "Epoch: 45  Loss: -65.44347381591797  Total Average Reward: -19.8\n",
            "Epoch: 46  Loss: 6.345224380493164  Total Average Reward: -19.4\n",
            "Epoch: 47  Loss: -20.664064407348633  Total Average Reward: -20.6\n",
            "Epoch: 48  Loss: -96.81982421875  Total Average Reward: -19.6\n",
            "Epoch: 49  Loss: -67.74040222167969  Total Average Reward: -19.8\n",
            "Epoch: 50  Loss: -36.56910705566406  Total Average Reward: -20.0\n",
            "Epoch: 51  Loss: -87.29061889648438  Total Average Reward: -19.6\n",
            "Epoch: 52  Loss: -36.27597427368164  Total Average Reward: -19.8\n",
            "Epoch: 53  Loss: -48.45610809326172  Total Average Reward: -20.0\n",
            "Epoch: 54  Loss: -50.90598678588867  Total Average Reward: -20.2\n",
            "Epoch: 55  Loss: -63.761234283447266  Total Average Reward: -19.4\n",
            "Epoch: 56  Loss: -15.206775665283203  Total Average Reward: -20.0\n",
            "Epoch: 57  Loss: -18.450349807739258  Total Average Reward: -19.2\n",
            "Epoch: 58  Loss: 32.93852615356445  Total Average Reward: -19.8\n",
            "Epoch: 59  Loss: -41.91907501220703  Total Average Reward: -19.2\n",
            "Epoch: 60  Loss: -3.027151107788086  Total Average Reward: -19.4\n",
            "Epoch: 61  Loss: 28.940521240234375  Total Average Reward: -19.2\n",
            "Epoch: 62  Loss: -81.35684204101562  Total Average Reward: -20.0\n",
            "Epoch: 63  Loss: -80.82714080810547  Total Average Reward: -19.8\n",
            "Epoch: 64  Loss: -56.18545150756836  Total Average Reward: -19.2\n",
            "Epoch: 65  Loss: -8.816390991210938  Total Average Reward: -18.8\n",
            "Epoch: 66  Loss: -47.298641204833984  Total Average Reward: -19.6\n",
            "Epoch: 67  Loss: -49.585208892822266  Total Average Reward: -17.8\n",
            "Epoch: 68  Loss: -145.79556274414062  Total Average Reward: -19.4\n",
            "Epoch: 69  Loss: -104.56942749023438  Total Average Reward: -18.8\n",
            "Epoch: 70  Loss: -48.90267562866211  Total Average Reward: -19.4\n",
            "Epoch: 71  Loss: 20.9494571685791  Total Average Reward: -18.8\n",
            "Epoch: 72  Loss: -67.20258331298828  Total Average Reward: -20.2\n",
            "Epoch: 73  Loss: -36.772117614746094  Total Average Reward: -19.4\n",
            "Epoch: 74  Loss: -28.998262405395508  Total Average Reward: -18.4\n",
            "Epoch: 75  Loss: -31.377586364746094  Total Average Reward: -19.4\n",
            "Epoch: 76  Loss: -79.5047607421875  Total Average Reward: -19.0\n",
            "Epoch: 77  Loss: -132.36782836914062  Total Average Reward: -20.0\n",
            "Epoch: 78  Loss: -153.58676147460938  Total Average Reward: -19.2\n",
            "Epoch: 79  Loss: -3.4383773803710938  Total Average Reward: -20.2\n",
            "Epoch: 80  Loss: 17.982147216796875  Total Average Reward: -18.8\n",
            "Epoch: 81  Loss: -49.84962844848633  Total Average Reward: -19.2\n",
            "Epoch: 82  Loss: -27.61278533935547  Total Average Reward: -18.6\n",
            "Epoch: 83  Loss: 24.41665267944336  Total Average Reward: -19.4\n",
            "Epoch: 84  Loss: -69.24964904785156  Total Average Reward: -19.4\n",
            "Epoch: 85  Loss: -66.7344970703125  Total Average Reward: -18.2\n",
            "Epoch: 86  Loss: -66.87690734863281  Total Average Reward: -19.4\n",
            "Epoch: 87  Loss: -65.0572509765625  Total Average Reward: -20.2\n",
            "Epoch: 88  Loss: -17.191917419433594  Total Average Reward: -17.6\n",
            "Epoch: 89  Loss: 26.58601188659668  Total Average Reward: -19.8\n",
            "Epoch: 90  Loss: 11.310134887695312  Total Average Reward: -19.2\n",
            "Epoch: 91  Loss: -57.00838851928711  Total Average Reward: -18.4\n",
            "Epoch: 92  Loss: -89.38751983642578  Total Average Reward: -18.2\n",
            "Epoch: 93  Loss: -86.4948501586914  Total Average Reward: -19.8\n",
            "Epoch: 94  Loss: 67.59326171875  Total Average Reward: -20.2\n",
            "Epoch: 95  Loss: -55.83149337768555  Total Average Reward: -20.0\n",
            "Epoch: 96  Loss: 67.24044036865234  Total Average Reward: -19.8\n",
            "Epoch: 97  Loss: -15.955873489379883  Total Average Reward: -18.6\n",
            "Epoch: 98  Loss: 1.5608930587768555  Total Average Reward: -18.4\n",
            "Epoch: 99  Loss: 21.01947784423828  Total Average Reward: -19.0\n",
            "Epoch: 100  Loss: 25.026382446289062  Total Average Reward: -19.6\n",
            "Epoch: 101  Loss: -17.149070739746094  Total Average Reward: -19.4\n",
            "Epoch: 102  Loss: -12.31655216217041  Total Average Reward: -17.4\n",
            "Epoch: 103  Loss: -65.55195617675781  Total Average Reward: -19.4\n",
            "Epoch: 104  Loss: -27.089454650878906  Total Average Reward: -18.8\n",
            "Epoch: 105  Loss: 52.063568115234375  Total Average Reward: -19.4\n",
            "Epoch: 106  Loss: 131.74728393554688  Total Average Reward: -20.0\n",
            "Epoch: 107  Loss: -174.23492431640625  Total Average Reward: -20.0\n",
            "Epoch: 108  Loss: 108.43208312988281  Total Average Reward: -19.0\n",
            "Epoch: 109  Loss: -3.532072067260742  Total Average Reward: -18.2\n",
            "Epoch: 110  Loss: -44.504032135009766  Total Average Reward: -20.0\n",
            "Epoch: 111  Loss: 94.46237182617188  Total Average Reward: -19.4\n",
            "Epoch: 112  Loss: -2.037384033203125  Total Average Reward: -20.2\n",
            "Epoch: 113  Loss: -9.858245849609375  Total Average Reward: -18.4\n",
            "Epoch: 114  Loss: -31.408679962158203  Total Average Reward: -18.8\n",
            "Epoch: 115  Loss: -75.37905883789062  Total Average Reward: -18.0\n",
            "Epoch: 116  Loss: -55.87955856323242  Total Average Reward: -16.2\n",
            "Epoch: 117  Loss: -92.9852294921875  Total Average Reward: -19.2\n",
            "Epoch: 118  Loss: 118.69141387939453  Total Average Reward: -16.6\n",
            "Epoch: 119  Loss: -31.632423400878906  Total Average Reward: -19.0\n",
            "Epoch: 120  Loss: 92.0870132446289  Total Average Reward: -18.6\n",
            "Epoch: 121  Loss: 68.42681884765625  Total Average Reward: -18.6\n",
            "Epoch: 122  Loss: -94.65792083740234  Total Average Reward: -19.0\n",
            "Epoch: 123  Loss: -71.67828369140625  Total Average Reward: -19.2\n",
            "Epoch: 124  Loss: -166.16790771484375  Total Average Reward: -17.2\n",
            "Epoch: 125  Loss: -49.51387023925781  Total Average Reward: -18.0\n",
            "Epoch: 126  Loss: 90.69442749023438  Total Average Reward: -18.4\n",
            "Epoch: 127  Loss: -131.3704376220703  Total Average Reward: -18.4\n",
            "Epoch: 128  Loss: 58.461570739746094  Total Average Reward: -18.4\n",
            "Epoch: 129  Loss: 19.47140121459961  Total Average Reward: -19.0\n",
            "Epoch: 130  Loss: -72.60814666748047  Total Average Reward: -17.6\n",
            "Epoch: 131  Loss: -144.19102478027344  Total Average Reward: -18.0\n",
            "Epoch: 132  Loss: 112.79070281982422  Total Average Reward: -18.4\n",
            "Epoch: 133  Loss: 89.95707702636719  Total Average Reward: -17.0\n",
            "Epoch: 134  Loss: -17.016700744628906  Total Average Reward: -18.8\n",
            "Epoch: 135  Loss: 27.336620330810547  Total Average Reward: -18.2\n",
            "Epoch: 136  Loss: -50.016876220703125  Total Average Reward: -18.0\n",
            "Epoch: 137  Loss: 44.33654022216797  Total Average Reward: -17.4\n",
            "Epoch: 138  Loss: 71.25360870361328  Total Average Reward: -17.2\n",
            "Epoch: 139  Loss: -98.70620727539062  Total Average Reward: -19.0\n",
            "Epoch: 140  Loss: 77.89495849609375  Total Average Reward: -19.2\n",
            "Epoch: 141  Loss: -46.692325592041016  Total Average Reward: -18.6\n",
            "Epoch: 142  Loss: 0.4204864501953125  Total Average Reward: -18.6\n",
            "Epoch: 143  Loss: -79.96672058105469  Total Average Reward: -17.4\n",
            "Epoch: 144  Loss: -73.08150482177734  Total Average Reward: -17.6\n",
            "Epoch: 145  Loss: 67.24886322021484  Total Average Reward: -17.8\n",
            "Epoch: 146  Loss: -85.14923858642578  Total Average Reward: -18.0\n",
            "Epoch: 147  Loss: -80.28410339355469  Total Average Reward: -18.6\n",
            "Epoch: 148  Loss: 14.053630828857422  Total Average Reward: -18.2\n",
            "Epoch: 149  Loss: -26.31173324584961  Total Average Reward: -18.2\n",
            "Epoch: 150  Loss: -102.93668365478516  Total Average Reward: -16.8\n",
            "Epoch: 151  Loss: -141.08087158203125  Total Average Reward: -17.6\n",
            "Epoch: 152  Loss: -252.64166259765625  Total Average Reward: -17.4\n",
            "Epoch: 153  Loss: 37.3251953125  Total Average Reward: -17.6\n",
            "Epoch: 154  Loss: 85.88203430175781  Total Average Reward: -19.0\n",
            "Epoch: 155  Loss: 16.823564529418945  Total Average Reward: -17.6\n",
            "Epoch: 156  Loss: -201.64491271972656  Total Average Reward: -16.8\n",
            "Epoch: 157  Loss: -40.451114654541016  Total Average Reward: -17.2\n",
            "Epoch: 158  Loss: -56.630035400390625  Total Average Reward: -18.0\n",
            "Epoch: 159  Loss: -88.69622802734375  Total Average Reward: -17.8\n",
            "Epoch: 160  Loss: 26.779388427734375  Total Average Reward: -18.6\n",
            "Epoch: 161  Loss: 41.63730239868164  Total Average Reward: -19.0\n",
            "Epoch: 162  Loss: 34.97340774536133  Total Average Reward: -16.8\n",
            "Epoch: 163  Loss: 10.421541213989258  Total Average Reward: -18.0\n",
            "Epoch: 164  Loss: -146.61407470703125  Total Average Reward: -17.8\n",
            "Epoch: 165  Loss: 84.17906188964844  Total Average Reward: -16.6\n",
            "Epoch: 166  Loss: -93.26066589355469  Total Average Reward: -18.0\n",
            "Epoch: 167  Loss: -7.2718505859375  Total Average Reward: -17.6\n",
            "Epoch: 168  Loss: -38.764583587646484  Total Average Reward: -16.6\n",
            "Epoch: 169  Loss: -51.22505187988281  Total Average Reward: -16.6\n",
            "Epoch: 170  Loss: -19.270553588867188  Total Average Reward: -18.0\n",
            "Epoch: 171  Loss: -216.67279052734375  Total Average Reward: -18.2\n",
            "Epoch: 172  Loss: -57.266197204589844  Total Average Reward: -18.0\n",
            "Epoch: 173  Loss: 1.4552726745605469  Total Average Reward: -19.0\n",
            "Epoch: 174  Loss: -11.607141494750977  Total Average Reward: -17.2\n",
            "Epoch: 175  Loss: -223.0026397705078  Total Average Reward: -17.2\n",
            "Epoch: 176  Loss: -42.419403076171875  Total Average Reward: -17.0\n",
            "Epoch: 177  Loss: -73.4558334350586  Total Average Reward: -17.2\n",
            "Epoch: 178  Loss: 26.918689727783203  Total Average Reward: -16.4\n",
            "Epoch: 179  Loss: -72.99394226074219  Total Average Reward: -17.6\n",
            "Epoch: 180  Loss: -33.47428894042969  Total Average Reward: -18.0\n",
            "Epoch: 181  Loss: 34.6736946105957  Total Average Reward: -16.8\n",
            "Epoch: 182  Loss: -49.16764450073242  Total Average Reward: -16.8\n",
            "Epoch: 183  Loss: 55.536346435546875  Total Average Reward: -15.6\n",
            "Epoch: 184  Loss: -106.03707885742188  Total Average Reward: -14.6\n",
            "Epoch: 185  Loss: 71.04762268066406  Total Average Reward: -17.4\n",
            "Epoch: 186  Loss: 68.97105407714844  Total Average Reward: -16.0\n",
            "Epoch: 187  Loss: 108.96331787109375  Total Average Reward: -15.4\n",
            "Epoch: 188  Loss: 60.871978759765625  Total Average Reward: -14.4\n",
            "Epoch: 189  Loss: -120.65414428710938  Total Average Reward: -16.6\n",
            "Epoch: 190  Loss: -26.67455291748047  Total Average Reward: -18.0\n",
            "Epoch: 191  Loss: -30.12677001953125  Total Average Reward: -15.8\n",
            "Epoch: 192  Loss: 2.2177200317382812  Total Average Reward: -17.0\n",
            "Epoch: 193  Loss: 157.94178771972656  Total Average Reward: -16.4\n",
            "Epoch: 194  Loss: 49.759979248046875  Total Average Reward: -15.4\n",
            "Epoch: 195  Loss: -82.6258773803711  Total Average Reward: -15.6\n",
            "Epoch: 196  Loss: 21.25005340576172  Total Average Reward: -15.8\n",
            "Epoch: 197  Loss: -81.4262924194336  Total Average Reward: -17.0\n",
            "Epoch: 198  Loss: -123.91162109375  Total Average Reward: -15.8\n",
            "Epoch: 199  Loss: -30.75821304321289  Total Average Reward: -18.2\n",
            "Epoch: 200  Loss: -20.991605758666992  Total Average Reward: -16.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-3676441c0e91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                   \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                   batch_size=5, render=False)\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-36-a7773d2cca22>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    109\u001b[0m                 }, self.model_file)\n\u001b[1;32m    110\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                 \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0;34m'port'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m       \u001b[0;34m'path'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m       \u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m   })\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: TypeError: Failed to fetch"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_5-lx3l5nJK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "c6437385-365e-4e12-b0d4-7336459f682f"
      },
      "source": [
        "!ls -al"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 15036\n",
            "drwxr-xr-x 1 root root     4096 Dec 25 08:44 .\n",
            "drwxr-xr-x 1 root root     4096 Dec 25 06:29 ..\n",
            "drwxr-xr-x 1 root root     4096 Dec 18 16:52 .config\n",
            "-rw-r--r-- 1 root root 15366555 Dec 25 08:40 model.pt\n",
            "drwxr-xr-x 3 root root     4096 Dec 25 07:26 runs\n",
            "-rw-r--r-- 1 root root     6869 Dec 25 08:44 runs.zip\n",
            "drwxr-xr-x 1 root root     4096 Dec 18 16:52 sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}