{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vpg.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "7S7uUulYtvEg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "19c33391-eaf4-43d5-e139-20220f1c0d29"
      },
      "cell_type": "code",
      "source": [
        "!pip install gym"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gym\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/22/4ff09745ade385ffe707fb5f053548f0f6a6e7d5e98a2b9d6c07f5b931a7/gym-0.10.9.tar.gz (1.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.5MB 9.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.14.6)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (2.18.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.11.0)\n",
            "Collecting pyglet>=1.2.0 (from gym)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/fc/dad5eaaab68f0c21e2f906a94ddb98175662cc5a654eee404d59554ce0fa/pyglet-1.3.2-py2.py3-none-any.whl (1.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.0MB 19.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2018.11.29)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (1.22)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym) (0.16.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Running setup.py bdist_wheel for gym ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/6c/3a/0e/b86dee98876bb56cdb482cc1f72201035e46d1baf69d10d028\n",
            "Successfully built gym\n",
            "Installing collected packages: pyglet, gym\n",
            "Successfully installed gym-0.10.9 pyglet-1.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "Iagalm01tqoq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import gym\n",
        "from collections import namedtuple\n",
        "from gym.wrappers import Monitor\n",
        "import scipy.signal\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z5pnT_RptqpD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "70ef8e82-0605-450b-fad4-6ff1c5ae8bb5"
      },
      "cell_type": "code",
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "obs_dim = env.observation_space.shape[0]\n",
        "num_actions = env.action_space.n\n",
        "hidden_layers = [64,64]\n",
        "epoch = 70\n",
        "steps_per_epoch = 4000\n",
        "gamma = 0.99\n",
        "gae_lambda = 0.97\n",
        "train_v = 60\n",
        "video_freq = 10\n",
        "save_freq = 5\n",
        "checkpointDir = \"checkpoint\"\n",
        "monitorDir = \"monitor\"\n",
        "obs_dim,num_actions"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "obNR_79mtqpP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def policy_estimator(x,action,hidden_layers,num_actions,output_activation,activation):\n",
        "    for l in hidden_layers:\n",
        "        x = tf.layers.dense(x,units=l,activation=activation)\n",
        "    logits = tf.layers.dense(x,units=num_actions,activation=output_activation)\n",
        "    \n",
        "    probs = tf.nn.log_softmax(logits)\n",
        "    pi_a = tf.squeeze(tf.multinomial(logits,1), axis=1)\n",
        "    prob_a = tf.reduce_sum(tf.one_hot(action, depth=num_actions) * probs, axis=1)\n",
        "    prob_pi = tf.reduce_sum(tf.one_hot(pi_a, depth=num_actions) * probs, axis=1)\n",
        "    return pi_a,prob_pi,prob_a"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ax-E7UHvtqpa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def value_estimator(x,hidden_layers,output_activation=None,activation=tf.tanh):\n",
        "    for l in hidden_layers:\n",
        "        x = tf.layers.dense(x,units=l,activation=activation)\n",
        "    logits = tf.layers.dense(x,units=1,activation=output_activation)\n",
        "    return tf.squeeze(logits,axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4cLItsiZtqpj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def actor_critic(x,act,hidden_layers,num_actions,output_activation=None,activation=tf.tanh):\n",
        "    pi_a,prob_pi,prob_a = policy_estimator(x,act,hidden_layers,num_actions,output_activation=None,activation=tf.tanh)\n",
        "    v = value_estimator(x,hidden_layers,output_activation=None,activation=tf.tanh)\n",
        "    return pi_a,prob_pi,prob_a,v"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5Y2ddW4gtqpt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = tf.placeholder(dtype = tf.float32,shape = (None,obs_dim),name=\"observations\")\n",
        "actions = tf.placeholder(dtype = tf.int32,shape = (None,),name=\"actions\")\n",
        "ret = tf.placeholder(dtype = tf.float32,shape = (None,),name=\"ret\")\n",
        "advantages = tf.placeholder(dtype = tf.float32,shape = (None,),name=\"advs\")\n",
        "\n",
        "pi_a,prob_pi,prob_a,value = actor_critic(x,actions,hidden_layers,num_actions)\n",
        "\n",
        "policy_loss = -tf.reduce_mean(prob_a * advantages)\n",
        "value_loss = tf.reduce_mean((ret - value)**2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xfMA8uJStqp1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimize_policy = tf.train.AdamOptimizer(learning_rate=3e-4).minimize(policy_loss)\n",
        "optimize_value = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(value_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zJF10qZqtqp_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def calculate_advantage(rews,values,final_value):\n",
        "    values = np.append(values,final_value)\n",
        "    rews = np.append(rews,final_value)\n",
        "    dels = rews[:-1] + gamma * values[1:] - values[:-1]\n",
        "    return scipy.signal.lfilter([1], [1, float(-gamma*gae_lambda)], dels[::-1], axis=0)[::-1]\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XoZww2OltqqI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def rewards_to_go(rews,final_value):\n",
        "    rews = np.append(rews,final_value)\n",
        "    return scipy.signal.lfilter([1], [1, float(-gamma)], rews[::-1], axis=0)[::-1][:-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GnNtPgEWtqqT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def update_policy(sess,obs_memory,action_memory,rtgs_memory,adv_memory):\n",
        "    feed_dict = {x:obs_memory,actions:action_memory,ret:rtgs_memory,advantages:adv_memory}\n",
        "    \n",
        "    policy_loss_e,value_loss_e = sess.run([policy_loss,value_loss],feed_dict=feed_dict)\n",
        "    \n",
        "    sess.run(optimize_policy,feed_dict=feed_dict)\n",
        "    \n",
        "    for i in range(train_v):\n",
        "        sess.run(optimize_value,feed_dict=feed_dict)\n",
        "\n",
        "    return policy_loss_e,value_loss_e\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E1Ms5U6stqqe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    sess = tf.Session()\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    env = gym.make('CartPole-v0')\n",
        "    \n",
        "    if not os.path.exists(checkpointDir):\n",
        "        os.makedirs(checkpointDir)\n",
        "    #if not os.path.exists(monitorDir):\n",
        "     #   os.makedirs(monitorDir)\n",
        "        \n",
        "    checkpoint = os.path.join(checkpointDir,\"model\")\n",
        "    #monitor = os.path.join(monitorDir,\"game\")\n",
        "    \n",
        "    saver = tf.train.Saver()\n",
        "        \n",
        "        \n",
        "    #env = Monitor(env, directory=monitor, video_callable=lambda e: e % \\\n",
        "    #                  video_freq == 0, resume=True)\n",
        "    \n",
        "    ckpt = tf.train.latest_checkpoint(checkpointDir)\n",
        "    if ckpt:\n",
        "        saver.restore(sess,ckpt)\n",
        "        print(\"Existing checkpoint {} restored...\".format(ckpt))\n",
        "            \n",
        "    obs,rew,done = env.reset(),0,False\n",
        "    total_rew = 0\n",
        "    episode_length = 0\n",
        "    episode_stats = []\n",
        "    losses = []\n",
        "    obs_memory = np.zeros((steps_per_epoch,obs_dim), dtype=np.float32)\n",
        "    action_memory = np.zeros(steps_per_epoch, dtype=np.int32)\n",
        "    rew_memory = np.zeros(steps_per_epoch, dtype=np.float32)\n",
        "    value_memory = np.zeros(steps_per_epoch, dtype=np.float32)\n",
        "    prob_pi_memory = np.zeros(steps_per_epoch, dtype=np.float32)\n",
        "    adv_memory = np.zeros(steps_per_epoch, dtype=np.float32)\n",
        "    rtgs_memory = np.zeros(steps_per_epoch, dtype=np.float32)\n",
        "\n",
        "    for e in range(epoch):\n",
        "        buf_head = 0\n",
        "        for t in range(steps_per_epoch):\n",
        "            pi_a_t,value_t,prob_pi_t = sess.run([pi_a,value,prob_pi],feed_dict={x:obs.reshape(1,-1)})\n",
        "            \n",
        "            obs_memory[t],action_memory[t],rew_memory[t],\\\n",
        "            value_memory[t],prob_pi_memory[t] = obs,pi_a_t,rew,value_t,prob_pi_t\n",
        "            \n",
        "            obs,rew,done,_ = env.step(pi_a_t[0])\n",
        "            total_rew += rew\n",
        "            episode_length += 1\n",
        "            \n",
        "            if done or (t==steps_per_epoch-1):\n",
        "                if not done:\n",
        "                    print(\"Alert:Final episode terminated without completion...\")\n",
        "                    final_value = sess.run([value],feed_dict={x:obs.reshape(1,-1)})\n",
        "                else:\n",
        "                    final_value = rew\n",
        "                \n",
        "                \n",
        "                adv_memory[buf_head:t] = calculate_advantage(rew_memory[buf_head:t],\\\n",
        "                                                             value_memory[buf_head:t],final_value)\n",
        "                rtgs_memory[buf_head:t] = rewards_to_go(rew_memory[buf_head:t],final_value)\n",
        "                buf_head = t\n",
        "                episode_stats.append([total_rew,episode_length])\n",
        "                obs,rew,done,total_rew,episode_length = env.reset(),0,False,0,0\n",
        "        \n",
        "        if(e%save_freq == 0) or (e == epoch-1):\n",
        "            saver.save(sess, checkpoint)\n",
        "            \n",
        "        policy_loss_e,value_loss_e = update_policy(sess,obs_memory,action_memory,rtgs_memory,adv_memory)\n",
        "        print(f\"\\n\\nEpoch : {e}\\nTotal Episodes : {len(episode_stats)}\\n\\\n",
        "Total rewards : {np.mean(episode_stats,axis=0)[0]}\\nAverage episode Length : {np.mean(episode_stats,axis=0)[1]}\")\n",
        "        \n",
        "        print(f\"Policy Loss : {policy_loss_e} Value Loss : {value_loss_e}\\n\")\n",
        "        episode_stats = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "_Rkwsq0ntqqn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10781
        },
        "outputId": "df4c1328-c67f-4c41-bad9-6fd613f04ff4"
      },
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 0\n",
            "Total Episodes : 161\n",
            "Total rewards : 24.84472049689441\n",
            "Average episode Length : 24.84472049689441\n",
            "Policy Loss : 7.7670698165893555 Value Loss : 360.4588928222656\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 1\n",
            "Total Episodes : 159\n",
            "Total rewards : 25.157232704402517\n",
            "Average episode Length : 25.157232704402517\n",
            "Policy Loss : 5.36539888381958 Value Loss : 275.8813171386719\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 2\n",
            "Total Episodes : 146\n",
            "Total rewards : 27.397260273972602\n",
            "Average episode Length : 27.397260273972602\n",
            "Policy Loss : 0.8333452939987183 Value Loss : 104.93696594238281\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 3\n",
            "Total Episodes : 154\n",
            "Total rewards : 25.974025974025974\n",
            "Average episode Length : 25.974025974025974\n",
            "Policy Loss : 0.15315032005310059 Value Loss : 115.03913116455078\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 4\n",
            "Total Episodes : 156\n",
            "Total rewards : 25.641025641025642\n",
            "Average episode Length : 25.641025641025642\n",
            "Policy Loss : 0.5209625363349915 Value Loss : 118.85281372070312\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 5\n",
            "Total Episodes : 148\n",
            "Total rewards : 27.027027027027028\n",
            "Average episode Length : 27.027027027027028\n",
            "Policy Loss : 0.4642765522003174 Value Loss : 127.3819580078125\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 6\n",
            "Total Episodes : 136\n",
            "Total rewards : 29.41176470588235\n",
            "Average episode Length : 29.41176470588235\n",
            "Policy Loss : 0.48115867376327515 Value Loss : 121.46157836914062\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 7\n",
            "Total Episodes : 125\n",
            "Total rewards : 32.0\n",
            "Average episode Length : 32.0\n",
            "Policy Loss : 0.922838032245636 Value Loss : 159.46441650390625\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 8\n",
            "Total Episodes : 133\n",
            "Total rewards : 30.075187969924812\n",
            "Average episode Length : 30.075187969924812\n",
            "Policy Loss : -0.311568945646286 Value Loss : 122.67050170898438\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 9\n",
            "Total Episodes : 129\n",
            "Total rewards : 31.007751937984494\n",
            "Average episode Length : 31.007751937984494\n",
            "Policy Loss : 0.521885871887207 Value Loss : 156.2285919189453\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 10\n",
            "Total Episodes : 129\n",
            "Total rewards : 31.007751937984494\n",
            "Average episode Length : 31.007751937984494\n",
            "Policy Loss : -0.17508471012115479 Value Loss : 130.2208709716797\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 11\n",
            "Total Episodes : 122\n",
            "Total rewards : 32.78688524590164\n",
            "Average episode Length : 32.78688524590164\n",
            "Policy Loss : 0.10997868329286575 Value Loss : 118.52400207519531\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 12\n",
            "Total Episodes : 112\n",
            "Total rewards : 35.714285714285715\n",
            "Average episode Length : 35.714285714285715\n",
            "Policy Loss : 0.7583785653114319 Value Loss : 152.57627868652344\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 13\n",
            "Total Episodes : 127\n",
            "Total rewards : 31.496062992125985\n",
            "Average episode Length : 31.496062992125985\n",
            "Policy Loss : -0.9446343779563904 Value Loss : 130.1900177001953\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 14\n",
            "Total Episodes : 102\n",
            "Total rewards : 39.21568627450981\n",
            "Average episode Length : 39.21568627450981\n",
            "Policy Loss : 1.4963350296020508 Value Loss : 224.1095733642578\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 15\n",
            "Total Episodes : 112\n",
            "Total rewards : 35.714285714285715\n",
            "Average episode Length : 35.714285714285715\n",
            "Policy Loss : -0.584027886390686 Value Loss : 202.4954833984375\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 16\n",
            "Total Episodes : 96\n",
            "Total rewards : 41.666666666666664\n",
            "Average episode Length : 41.666666666666664\n",
            "Policy Loss : 0.6791509985923767 Value Loss : 187.71209716796875\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 17\n",
            "Total Episodes : 103\n",
            "Total rewards : 38.83495145631068\n",
            "Average episode Length : 38.83495145631068\n",
            "Policy Loss : -0.4402691125869751 Value Loss : 195.92164611816406\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 18\n",
            "Total Episodes : 107\n",
            "Total rewards : 37.38317757009346\n",
            "Average episode Length : 37.38317757009346\n",
            "Policy Loss : -0.5395572781562805 Value Loss : 148.80628967285156\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 19\n",
            "Total Episodes : 106\n",
            "Total rewards : 37.735849056603776\n",
            "Average episode Length : 37.735849056603776\n",
            "Policy Loss : -0.04325598105788231 Value Loss : 164.10836791992188\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 20\n",
            "Total Episodes : 99\n",
            "Total rewards : 40.4040404040404\n",
            "Average episode Length : 40.4040404040404\n",
            "Policy Loss : 0.5418989658355713 Value Loss : 199.06484985351562\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 21\n",
            "Total Episodes : 76\n",
            "Total rewards : 52.63157894736842\n",
            "Average episode Length : 52.63157894736842\n",
            "Policy Loss : 1.4610563516616821 Value Loss : 281.76080322265625\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 22\n",
            "Total Episodes : 98\n",
            "Total rewards : 40.816326530612244\n",
            "Average episode Length : 40.816326530612244\n",
            "Policy Loss : -1.9379968643188477 Value Loss : 225.4759063720703\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 23\n",
            "Total Episodes : 88\n",
            "Total rewards : 45.45454545454545\n",
            "Average episode Length : 45.45454545454545\n",
            "Policy Loss : 0.19243961572647095 Value Loss : 205.53233337402344\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 24\n",
            "Total Episodes : 97\n",
            "Total rewards : 41.23711340206186\n",
            "Average episode Length : 41.23711340206186\n",
            "Policy Loss : -0.9762048125267029 Value Loss : 173.19480895996094\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 25\n",
            "Total Episodes : 91\n",
            "Total rewards : 43.956043956043956\n",
            "Average episode Length : 43.956043956043956\n",
            "Policy Loss : -0.07410962134599686 Value Loss : 181.45718383789062\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 26\n",
            "Total Episodes : 91\n",
            "Total rewards : 43.956043956043956\n",
            "Average episode Length : 43.956043956043956\n",
            "Policy Loss : -0.15701724588871002 Value Loss : 175.47586059570312\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 27\n",
            "Total Episodes : 95\n",
            "Total rewards : 42.10526315789474\n",
            "Average episode Length : 42.10526315789474\n",
            "Policy Loss : -0.41853222250938416 Value Loss : 195.2838134765625\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 28\n",
            "Total Episodes : 81\n",
            "Total rewards : 49.382716049382715\n",
            "Average episode Length : 49.382716049382715\n",
            "Policy Loss : 0.8518103361129761 Value Loss : 194.6254119873047\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 29\n",
            "Total Episodes : 84\n",
            "Total rewards : 47.61904761904762\n",
            "Average episode Length : 47.61904761904762\n",
            "Policy Loss : -0.05199645832180977 Value Loss : 210.73265075683594\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 30\n",
            "Total Episodes : 84\n",
            "Total rewards : 47.61904761904762\n",
            "Average episode Length : 47.61904761904762\n",
            "Policy Loss : -0.059681616723537445 Value Loss : 228.47828674316406\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 31\n",
            "Total Episodes : 78\n",
            "Total rewards : 51.282051282051285\n",
            "Average episode Length : 51.282051282051285\n",
            "Policy Loss : -0.0935634970664978 Value Loss : 265.8684387207031\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 32\n",
            "Total Episodes : 77\n",
            "Total rewards : 51.94805194805195\n",
            "Average episode Length : 51.94805194805195\n",
            "Policy Loss : -0.5773141980171204 Value Loss : 244.87350463867188\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 33\n",
            "Total Episodes : 81\n",
            "Total rewards : 49.382716049382715\n",
            "Average episode Length : 49.382716049382715\n",
            "Policy Loss : -0.3026859760284424 Value Loss : 238.82843017578125\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 34\n",
            "Total Episodes : 69\n",
            "Total rewards : 57.971014492753625\n",
            "Average episode Length : 57.971014492753625\n",
            "Policy Loss : 0.4682352840900421 Value Loss : 232.66064453125\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 35\n",
            "Total Episodes : 78\n",
            "Total rewards : 51.282051282051285\n",
            "Average episode Length : 51.282051282051285\n",
            "Policy Loss : -0.6883708238601685 Value Loss : 234.74990844726562\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 36\n",
            "Total Episodes : 72\n",
            "Total rewards : 55.55555555555556\n",
            "Average episode Length : 55.55555555555556\n",
            "Policy Loss : -0.16288697719573975 Value Loss : 223.31507873535156\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 37\n",
            "Total Episodes : 72\n",
            "Total rewards : 55.55555555555556\n",
            "Average episode Length : 55.55555555555556\n",
            "Policy Loss : -0.3083688020706177 Value Loss : 235.00819396972656\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 38\n",
            "Total Episodes : 57\n",
            "Total rewards : 70.17543859649123\n",
            "Average episode Length : 70.17543859649123\n",
            "Policy Loss : 1.5301291942596436 Value Loss : 365.0758056640625\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 39\n",
            "Total Episodes : 62\n",
            "Total rewards : 64.51612903225806\n",
            "Average episode Length : 64.51612903225806\n",
            "Policy Loss : -1.1262494325637817 Value Loss : 250.8930206298828\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 40\n",
            "Total Episodes : 74\n",
            "Total rewards : 54.054054054054056\n",
            "Average episode Length : 54.054054054054056\n",
            "Policy Loss : -1.7211904525756836 Value Loss : 244.8134002685547\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 41\n",
            "Total Episodes : 58\n",
            "Total rewards : 68.96551724137932\n",
            "Average episode Length : 68.96551724137932\n",
            "Policy Loss : 1.5539981126785278 Value Loss : 317.76611328125\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 42\n",
            "Total Episodes : 66\n",
            "Total rewards : 60.60606060606061\n",
            "Average episode Length : 60.60606060606061\n",
            "Policy Loss : -1.581821322441101 Value Loss : 268.1042175292969\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 43\n",
            "Total Episodes : 65\n",
            "Total rewards : 61.53846153846154\n",
            "Average episode Length : 61.53846153846154\n",
            "Policy Loss : -0.19506986439228058 Value Loss : 246.45347595214844\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 44\n",
            "Total Episodes : 55\n",
            "Total rewards : 72.72727272727273\n",
            "Average episode Length : 72.72727272727273\n",
            "Policy Loss : 1.243331789970398 Value Loss : 324.466064453125\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 45\n",
            "Total Episodes : 53\n",
            "Total rewards : 75.47169811320755\n",
            "Average episode Length : 75.47169811320755\n",
            "Policy Loss : -0.0807805210351944 Value Loss : 298.0130310058594\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 46\n",
            "Total Episodes : 54\n",
            "Total rewards : 74.07407407407408\n",
            "Average episode Length : 74.07407407407408\n",
            "Policy Loss : -1.3077219724655151 Value Loss : 238.70960998535156\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 47\n",
            "Total Episodes : 60\n",
            "Total rewards : 66.66666666666667\n",
            "Average episode Length : 66.66666666666667\n",
            "Policy Loss : -0.64748615026474 Value Loss : 287.72802734375\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 48\n",
            "Total Episodes : 60\n",
            "Total rewards : 66.66666666666667\n",
            "Average episode Length : 66.66666666666667\n",
            "Policy Loss : -0.324465811252594 Value Loss : 238.65895080566406\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 49\n",
            "Total Episodes : 53\n",
            "Total rewards : 75.47169811320755\n",
            "Average episode Length : 75.47169811320755\n",
            "Policy Loss : 0.13567763566970825 Value Loss : 223.61712646484375\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 50\n",
            "Total Episodes : 46\n",
            "Total rewards : 86.95652173913044\n",
            "Average episode Length : 86.95652173913044\n",
            "Policy Loss : 1.0167876482009888 Value Loss : 322.16168212890625\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 51\n",
            "Total Episodes : 60\n",
            "Total rewards : 66.66666666666667\n",
            "Average episode Length : 66.66666666666667\n",
            "Policy Loss : -2.4323182106018066 Value Loss : 365.0172119140625\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 52\n",
            "Total Episodes : 49\n",
            "Total rewards : 81.63265306122449\n",
            "Average episode Length : 81.63265306122449\n",
            "Policy Loss : 0.39964759349823 Value Loss : 316.8972473144531\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 53\n",
            "Total Episodes : 48\n",
            "Total rewards : 83.33333333333333\n",
            "Average episode Length : 83.33333333333333\n",
            "Policy Loss : -0.2897120416164398 Value Loss : 283.1358642578125\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 54\n",
            "Total Episodes : 43\n",
            "Total rewards : 93.02325581395348\n",
            "Average episode Length : 93.02325581395348\n",
            "Policy Loss : 1.2180691957473755 Value Loss : 308.32666015625\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 55\n",
            "Total Episodes : 49\n",
            "Total rewards : 81.63265306122449\n",
            "Average episode Length : 81.63265306122449\n",
            "Policy Loss : -1.5722190141677856 Value Loss : 277.3755187988281\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 56\n",
            "Total Episodes : 45\n",
            "Total rewards : 88.88888888888889\n",
            "Average episode Length : 88.88888888888889\n",
            "Policy Loss : 0.21438615024089813 Value Loss : 220.8236846923828\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 57\n",
            "Total Episodes : 52\n",
            "Total rewards : 76.92307692307692\n",
            "Average episode Length : 76.92307692307692\n",
            "Policy Loss : -0.7092792987823486 Value Loss : 242.0998992919922\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 58\n",
            "Total Episodes : 43\n",
            "Total rewards : 93.02325581395348\n",
            "Average episode Length : 93.02325581395348\n",
            "Policy Loss : 1.1367428302764893 Value Loss : 266.3800354003906\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 59\n",
            "Total Episodes : 43\n",
            "Total rewards : 93.02325581395348\n",
            "Average episode Length : 93.02325581395348\n",
            "Policy Loss : -0.5224692225456238 Value Loss : 229.65943908691406\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 60\n",
            "Total Episodes : 49\n",
            "Total rewards : 81.63265306122449\n",
            "Average episode Length : 81.63265306122449\n",
            "Policy Loss : -1.355635643005371 Value Loss : 344.0330505371094\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 61\n",
            "Total Episodes : 45\n",
            "Total rewards : 88.88888888888889\n",
            "Average episode Length : 88.88888888888889\n",
            "Policy Loss : -0.06225043535232544 Value Loss : 318.71722412109375\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 62\n",
            "Total Episodes : 41\n",
            "Total rewards : 97.5609756097561\n",
            "Average episode Length : 97.5609756097561\n",
            "Policy Loss : 0.7750517725944519 Value Loss : 290.73394775390625\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 63\n",
            "Total Episodes : 42\n",
            "Total rewards : 95.23809523809524\n",
            "Average episode Length : 95.23809523809524\n",
            "Policy Loss : -0.03935714066028595 Value Loss : 305.5685119628906\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 64\n",
            "Total Episodes : 40\n",
            "Total rewards : 100.0\n",
            "Average episode Length : 100.0\n",
            "Policy Loss : -0.489282488822937 Value Loss : 347.4638977050781\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 65\n",
            "Total Episodes : 38\n",
            "Total rewards : 105.26315789473684\n",
            "Average episode Length : 105.26315789473684\n",
            "Policy Loss : -0.34817802906036377 Value Loss : 271.1178894042969\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 66\n",
            "Total Episodes : 41\n",
            "Total rewards : 97.5609756097561\n",
            "Average episode Length : 97.5609756097561\n",
            "Policy Loss : -0.8307756185531616 Value Loss : 321.2264709472656\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 67\n",
            "Total Episodes : 40\n",
            "Total rewards : 100.0\n",
            "Average episode Length : 100.0\n",
            "Policy Loss : 0.3322867453098297 Value Loss : 355.22003173828125\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 68\n",
            "Total Episodes : 36\n",
            "Total rewards : 111.11111111111111\n",
            "Average episode Length : 111.11111111111111\n",
            "Policy Loss : -0.12085147947072983 Value Loss : 311.7611999511719\n",
            "\n",
            "Alert:Final episode terminated without completion...\n",
            "\n",
            "\n",
            "Epoch : 69\n",
            "Total Episodes : 36\n",
            "Total rewards : 111.11111111111111\n",
            "Average episode Length : 111.11111111111111\n",
            "Policy Loss : 0.34788477420806885 Value Loss : 310.70208740234375\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2lpvs0zft2MK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('checkpoint/checkpoint')\n",
        "files.download('checkpoint/model.data-00000-of-00001')\n",
        "files.download('checkpoint/model.index')\n",
        "files.download('checkpoint/model.meta')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}