{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ninja/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "from collections import namedtuple\n",
    "from gym.wrappers import Monitor\n",
    "import scipy.signal\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, 2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "hidden_layers = [64,64]\n",
    "epoch = 50\n",
    "steps_per_epoch = 4000\n",
    "gamma = 0.99\n",
    "gae_lambda = 0.97\n",
    "train_v = 60\n",
    "video_freq = 10\n",
    "save_freq = 5\n",
    "checkpointDir = \"checkpoint\"\n",
    "monitorDir = \"monitor\"\n",
    "obs_dim,num_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_estimator(x,action,hidden_layers,num_actions,output_activation,activation):\n",
    "    for l in hidden_layers:\n",
    "        x = tf.layers.dense(x,units=l,activation=activation)\n",
    "    logits = tf.layers.dense(x,units=num_actions,activation=output_activation)\n",
    "    \n",
    "    probs = tf.nn.log_softmax(logits)\n",
    "    pi_a = tf.squeeze(tf.multinomial(logits,1), axis=1)\n",
    "    prob_a = tf.reduce_sum(tf.one_hot(action, depth=num_actions) * probs, axis=1)\n",
    "    prob_pi = tf.reduce_sum(tf.one_hot(pi_a, depth=num_actions) * probs, axis=1)\n",
    "    return pi_a,prob_pi,prob_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def value_estimator(x,hidden_layers,output_activation=None,activation=tf.tanh):\n",
    "    for l in hidden_layers:\n",
    "        x = tf.layers.dense(x,units=l,activation=activation)\n",
    "    logits = tf.layers.dense(x,units=1,activation=output_activation)\n",
    "    return tf.squeeze(logits,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def actor_critic(x,act,hidden_layers,num_actions,output_activation=None,activation=tf.tanh):\n",
    "    pi_a,prob_pi,prob_a = policy_estimator(x,act,hidden_layers,num_actions,output_activation=None,activation=tf.tanh)\n",
    "    v = value_estimator(x,hidden_layers,output_activation=None,activation=tf.tanh)\n",
    "    return pi_a,prob_pi,prob_a,v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(dtype = tf.float32,shape = (None,obs_dim),name=\"observations\")\n",
    "actions = tf.placeholder(dtype = tf.int32,shape = (None,),name=\"actions\")\n",
    "ret = tf.placeholder(dtype = tf.float32,shape = (None,),name=\"ret\")\n",
    "advantages = tf.placeholder(dtype = tf.float32,shape = (None,),name=\"advs\")\n",
    "\n",
    "pi_a,prob_pi,prob_a,value = actor_critic(x,actions,hidden_layers,num_actions)\n",
    "\n",
    "policy_loss = -tf.reduce_mean(prob_a * advantages)\n",
    "value_loss = tf.reduce_mean((ret - value)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimize_policy = tf.train.AdamOptimizer(learning_rate=3e-4).minimize(policy_loss)\n",
    "optimize_value = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(value_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_advantage(rews,values,final_value):\n",
    "    values = np.append(values,final_value)\n",
    "    rews = np.append(rews,final_value)\n",
    "    dels = rews[:-1] + gamma * values[1:] - values[:-1]\n",
    "    return scipy.signal.lfilter([1], [1, float(-gamma*gae_lambda)], dels[::-1], axis=0)[::-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rewards_to_go(rews,final_value):\n",
    "    rews = np.append(rews,final_value)\n",
    "    return scipy.signal.lfilter([1], [1, float(-gamma)], rews[::-1], axis=0)[::-1][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_policy(sess,obs_memory,action_memory,rtgs_memory,adv_memory):\n",
    "    feed_dict = {x:obs_memory,actions:action_memory,ret:rtgs_memory,advantages:adv_memory}\n",
    "    \n",
    "    policy_loss_e,value_loss_e = sess.run([policy_loss,value_loss],feed_dict=feed_dict)\n",
    "    \n",
    "    sess.run(optimize_policy,feed_dict=feed_dict)\n",
    "    \n",
    "    for i in range(train_v):\n",
    "        sess.run(optimize_value,feed_dict=feed_dict)\n",
    "\n",
    "    return policy_loss_e,value_loss_e\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    env = gym.make('CartPole-v0')\n",
    "    \n",
    "    if not os.path.exists(checkpointDir):\n",
    "        os.makedirs(checkpointDir)\n",
    "    #if not os.path.exists(monitorDir):\n",
    "     #   os.makedirs(monitorDir)\n",
    "        \n",
    "    checkpoint = os.path.join(checkpointDir,\"model\")\n",
    "    #monitor = os.path.join(monitorDir,\"game\")\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "        \n",
    "        \n",
    "    #env = Monitor(env, directory=monitor, video_callable=lambda e: e % \\\n",
    "    #                  video_freq == 0, resume=True)\n",
    "    \n",
    "    ckpt = tf.train.latest_checkpoint(checkpointDir)\n",
    "    if ckpt:\n",
    "        saver.restore(sess,ckpt)\n",
    "        print(\"Existing checkpoint {} restored...\".format(ckpt))\n",
    "            \n",
    "    obs,rew,done = env.reset(),0,False\n",
    "    total_rew = 0\n",
    "    episode_length = 0\n",
    "    episode_stats = []\n",
    "    losses = []\n",
    "    obs_memory = np.zeros((steps_per_epoch,obs_dim), dtype=np.float32)\n",
    "    action_memory = np.zeros(steps_per_epoch, dtype=np.int32)\n",
    "    rew_memory = np.zeros(steps_per_epoch, dtype=np.float32)\n",
    "    value_memory = np.zeros(steps_per_epoch, dtype=np.float32)\n",
    "    prob_pi_memory = np.zeros(steps_per_epoch, dtype=np.float32)\n",
    "    adv_memory = np.zeros(steps_per_epoch, dtype=np.float32)\n",
    "    rtgs_memory = np.zeros(steps_per_epoch, dtype=np.float32)\n",
    "\n",
    "    for e in range(epoch):\n",
    "        buf_head = 0\n",
    "        for t in range(steps_per_epoch):\n",
    "            pi_a_t,value_t,prob_pi_t = sess.run([pi_a,value,prob_pi],feed_dict={x:obs.reshape(1,-1)})\n",
    "            \n",
    "            obs_memory[t],action_memory[t],rew_memory[t],\\\n",
    "            value_memory[t],prob_pi_memory[t] = obs,pi_a_t,rew,value_t,prob_pi_t\n",
    "            \n",
    "            obs,rew,done,_ = env.step(pi_a_t[0])\n",
    "            total_rew += rew\n",
    "            episode_length += 1\n",
    "            \n",
    "            if done or (t==steps_per_epoch-1):\n",
    "                if not done:\n",
    "                    print(\"Alert:Final episode terminated without completion...\")\n",
    "                    final_value = sess.run([value],feed_dict={x:obs.reshape(1,-1)})\n",
    "                else:\n",
    "                    final_value = rew\n",
    "                \n",
    "                \n",
    "                adv_memory[buf_head:t] = calculate_advantage(rew_memory[buf_head:t],\\\n",
    "                                                             value_memory[buf_head:t],final_value)\n",
    "                rtgs_memory[buf_head:t] = rewards_to_go(rew_memory[buf_head:t],final_value)\n",
    "                buf_head = t\n",
    "                episode_stats.append([total_rew,episode_length])\n",
    "                obs,rew,done,total_rew,episode_length = env.reset(),0,False,0,0\n",
    "        \n",
    "        if(e%save_freq == 0) or (e == epoch-1):\n",
    "            saver.save(sess, checkpoint)\n",
    "            \n",
    "        policy_loss_e,value_loss_e = update_policy(sess,obs_memory,action_memory,rtgs_memory,adv_memory)\n",
    "        print(f\"\\n\\nEpoch : {e}\\nTotal Episodes : {len(episode_stats)}\\n\\\n",
    "Total rewards : {np.mean(episode_stats,axis=0)[0]}\\nAverage episode Length : {np.mean(episode_stats,axis=0)[1]}\")\n",
    "        \n",
    "        print(f\"Policy Loss : {policy_loss_e} Value Loss : {value_loss_e}\\n\")\n",
    "        episode_stats = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from checkpoint/model\n",
      "Existing checkpoint checkpoint/model restored...\n",
      "Alert:Final episode terminated without completion...\n",
      "\n",
      "\n",
      "Epoch : 0\n",
      "Total Episodes : 198\n",
      "Total rewards : 20.2020202020202\n",
      "Average episode Length : 20.2020202020202\n",
      "Policy Loss : 6.782705307006836 Value Loss : 220.69940185546875\n",
      "\n",
      "Alert:Final episode terminated without completion...\n",
      "\n",
      "\n",
      "Epoch : 1\n",
      "Total Episodes : 191\n",
      "Total rewards : 20.94240837696335\n",
      "Average episode Length : 20.94240837696335\n",
      "Policy Loss : 3.926971435546875 Value Loss : 134.0321502685547\n",
      "\n",
      "Alert:Final episode terminated without completion...\n",
      "\n",
      "\n",
      "Epoch : 2\n",
      "Total Episodes : 184\n",
      "Total rewards : 21.73913043478261\n",
      "Average episode Length : 21.73913043478261\n",
      "Policy Loss : 0.2852747142314911 Value Loss : 81.26679992675781\n",
      "\n",
      "Alert:Final episode terminated without completion...\n",
      "\n",
      "\n",
      "Epoch : 3\n",
      "Total Episodes : 181\n",
      "Total rewards : 22.099447513812155\n",
      "Average episode Length : 22.099447513812155\n",
      "Policy Loss : 0.566409707069397 Value Loss : 75.23833465576172\n",
      "\n",
      "Alert:Final episode terminated without completion...\n",
      "\n",
      "\n",
      "Epoch : 4\n",
      "Total Episodes : 166\n",
      "Total rewards : 24.096385542168676\n",
      "Average episode Length : 24.096385542168676\n",
      "Policy Loss : 0.8855252861976624 Value Loss : 81.93799591064453\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
